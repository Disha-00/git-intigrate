{"cells":[{"cell_type":"markdown","source":["**<mark>Download from the web</mark>**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1a582432-60a2-42d0-84f8-7b276c06e55a"},{"cell_type":"code","source":["Nbr_Files_to_Download = 700"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b4fdcaf1-09c8-45b0-a7f9-34ba0422f130"},{"cell_type":"code","source":["import pandas as pd\n","import re ,shutil\n","from urllib.request import urlopen\n","import os\n","import requests\n","import pyarrow.dataset as ds\n","import pyarrow.parquet as pq\n","import pyarrow as pa\n","def download(url,Path,x):\n","    #run only once to create an empty log file\n","    if not os.path.exists(Path):\n","      os.makedirs(Path, exist_ok=True) \n","      os.makedirs(Path+\"_log\", exist_ok=True)\n","      log_tb = pa.Table.from_pylist( ['x'], schema=pa.schema({ \"file\" : pa.string()}))\n","      pq.write_table(log_tb,Path+\"_log/log.parquet\")  \n","    # Regex don't ask I just copy it\n","    result = urlopen(url).read().decode('utf-8')\n","    pattern = re.compile(r'[\\w.]*.zip')\n","    filelist1 = pattern.findall(result)\n","    filelist_unique = dict.fromkeys(filelist1)\n","    filelist_sorted=sorted(filelist_unique, reverse=True)\n","    filelist = filelist_sorted[:x]\n","    ### Read from existing log\n","    df = ds.dataset(Path + \"_log/log.parquet\").to_table().to_pandas()     \n","    file_loaded= df['file'].unique()\n","    current = file_loaded.tolist()\n","    files_to_upload = list(set(filelist) - set(current))\n","    files_to_upload = list(dict.fromkeys(files_to_upload)) \n","    print(str(len(files_to_upload)) + ' New File Loaded')\n","    if len(files_to_upload) != 0 :\n","      for x in files_to_upload:\n","           with requests.get(url+x, stream=True) as resp:\n","            if resp.ok:\n","              with open(f\"{Path}/{x}\", \"wb\") as f:\n","               for chunk in resp.iter_content(chunk_size=4096):\n","                f.write(chunk)\n","      existing_file = pd.DataFrame( file_loaded)\n","      new_file = pd.DataFrame(  files_to_upload)\n","      log = pd.concat ([new_file,existing_file], ignore_index=True)\n","      #print(log)\n","      log.rename(columns={0: 'file'}, inplace=True)\n","      log_tb=pa.Table.from_pandas(log,preserve_index=False)\n","      #print(log_tb)\n","      log_schema = pa.schema([pa.field('file', pa.string())])\n","      log_tb=log_tb.cast(target_schema=log_schema)\n","      pq.write_table(log_tb,Path+\"_log/log.parquet\")\n","      return \"done\"\n","    else:\n","     return \"nothing to see here\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"28416977-0f39-41ca-aa84-9a5d42a6e519"},{"cell_type":"code","source":["download(\"http://nemweb.com.au/Reports/Current/DispatchIS_Reports/\",\"/lakehouse/default/Files/0_Source/Current/DispatchIS_Reports/\",Nbr_Files_to_Download)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e13d5328-043a-47ee-8fcb-1b4825bbf8b8"},{"cell_type":"code","source":["download(\"http://nemweb.com.au/Reports/Current/Dispatch_SCADA/\",\"/lakehouse/default/Files/0_Source/Current/Dispatch_SCADA/\",Nbr_Files_to_Download)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"353a087d-e486-4262-be4f-8479b0149fa7"},{"cell_type":"markdown","source":["**<mark>Unzip</mark>**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9d3164ad-7f2a-42b9-8bcf-c8344d23860b"},{"cell_type":"code","source":["import pandas as pd\n","from shutil import unpack_archive\n","import os\n","import pyarrow.dataset as ds\n","import pyarrow.parquet as pq\n","import pyarrow as pa\n","def unzip(Source, Destination):\n","    #run only once to create an empty log file\n","    if not os.path.exists(Destination):\n","      os.makedirs(Destination, exist_ok=True) \n","      os.makedirs(Destination+\"_log\", exist_ok=True)\n","      log_tb = pa.Table.from_pylist( ['x'], schema=pa.schema({ \"file\" : pa.string()}))\n","      pq.write_table(log_tb,Destination+\"_log/log.parquet\")  \n","    # check zip files\n","    df = ds.dataset(Source + \"_log/log.parquet\").to_table().to_pandas()     \n","    filelist_unique= df['file'].unique()\n","    filelist=filelist_unique.tolist()\n","\n","    ### Read from existing log\n","    df = ds.dataset(Destination + \"_log/log.parquet\").to_table().to_pandas()     \n","    file_loaded= df['file'].unique()\n","    current = file_loaded.tolist()\n","    files_to_upload = list(set(filelist) - set(current))\n","    files_to_upload = list(dict.fromkeys(files_to_upload)) \n","    print(str(len(files_to_upload)) + ' New File Loaded')\n","    if len(files_to_upload) != 0 :\n","      for x in files_to_upload:\n","          try:\n","            unpack_archive(str(Source+x), str(Destination), 'zip')\n","          except:\n","            pass\n","      existing_file = pd.DataFrame( file_loaded)\n","      new_file = pd.DataFrame(  files_to_upload)\n","      log = pd.concat ([new_file,existing_file], ignore_index=True)\n","      #print(log)\n","      log.rename(columns={0: 'file'}, inplace=True)\n","      log_tb=pa.Table.from_pandas(log,preserve_index=False)\n","      #print(log_tb)\n","      log_schema = pa.schema([pa.field('file', pa.string())])\n","      log_tb=log_tb.cast(target_schema=log_schema)\n","      pq.write_table(log_tb,Destination+\"_log/log.parquet\")\n","      return \"done\"\n","    else:\n","     return \"nothing to see here\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2c7b2063-0ead-436c-9dd8-84be59629a35"},{"cell_type":"code","source":["unzip(\"/lakehouse/default/Files/0_Source/Current/Dispatch_SCADA/\",\"/lakehouse/default/Files/1_Transform/0/Current/Dispatch_SCADA/\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"24b682e6-46c4-43b4-b64c-10728eb795f5"},{"cell_type":"code","source":["unzip(\"/lakehouse/default/Files/0_Source/Current/DispatchIS_Reports/\",\"/lakehouse/default/Files/1_Transform/0/Current/DispatchIS_Reports/\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"750aa97d-40f9-43e8-aa54-b13427dd41e7"},{"cell_type":"markdown","source":["**<mark>Transform</mark>**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8c62159f-0a1b-42eb-bd3f-80a023ffadd4"},{"cell_type":"markdown","source":["_**SCADA**_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1d8575da-edc7-481e-aebb-f0950a8839c6"},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import time\n","from delta.tables import *\n","def extract_scada(Path,files_to_upload) :\n"," appended_data = []\n"," for filename in files_to_upload:\n","  try:\n","    df = pd.read_csv(Path+filename, skiprows=1)\n","    df=df.dropna(how='all') #drop na\n","    df['SETTLEMENTDATE']= pd.to_datetime(df['SETTLEMENTDATE'])\n","    df['DATE']= df['SETTLEMENTDATE'].dt.date\n","    df = df[df.I != \"C\"]\n","    df = pd.DataFrame(df, columns=['SETTLEMENTDATE','DUID','SCADAVALUE','DATE'])\n","    df=df.rename(columns={\"SCADAVALUE\": \"INITIALMW\"})\n","    df['INTERVENTION'] = 0.0\n","    df['PRIORITY'] =0\n","    df['PRIORITY'] = df['PRIORITY'].astype(np.int32)\n","    df['year'] = df['SETTLEMENTDATE'].dt.year\n","    df['file'] = filename\n","    appended_data.append(df)\n","  except:\n","    pass\n"," appended_data = pd.concat(appended_data,ignore_index=True)\n"," appended_data = appended_data.replace(to_replace='None', value=np.nan).dropna()\n"," return appended_data"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3612ad9a-2fc5-4235-97e0-56ce44d12803"},{"cell_type":"code","source":["existing_files=spark.sql(\"\"\" select distinct file as file from scada where PRIORITY = 0 and year >= 2024 \"\"\").toPandas()['file'].tolist()\n","len(existing_files)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4695e129-fed8-487c-bdf7-e004c282cff9"},{"cell_type":"code","source":["Path = \"/lakehouse/default/Files/1_Transform/0/Current/Dispatch_SCADA/\"\n","df = pd.read_parquet(Path+\"_log/log.parquet\")\n","df = df.replace(to_replace='None', value=np.nan).dropna()\n","list_files = df['file'].tolist()\n","filelist_csv = [w.replace('.zip', '.CSV') for w in list_files]\n","files_to_upload = list(set(filelist_csv) - set(existing_files))\n","files_to_upload = list(dict.fromkeys(files_to_upload))\n","print(len(files_to_upload))\n","########################### Write Data ##########################################################\n","if len(files_to_upload) >0 :\n","    df=spark.createDataFrame(extract_scada(Path,files_to_upload))\n","    #display(df)\n","    df.write.mode(\"append\").format(\"delta\").partitionBy(\"year\").saveAsTable(\"scada\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"ad037a49-970b-42ed-bb0f-d9f5dab89356"},{"cell_type":"markdown","source":["_**PRICE**_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0e0ad843-1901-4e84-86bf-59fafadbfd14"},{"cell_type":"code","source":["def extract_price(Path,files_to_upload) :\n"," appended_data = []\n"," for filename in files_to_upload:\n","   try:\n","    df = pd.read_csv(Path+filename,skiprows=1,dtype=str,names=range(109),usecols=range(10))\n","    df.columns = df.iloc[0]\n","    df = df[1:]\n","    df=df.query('CASE_SOLUTION==\"PRICE\"')\n","    df.columns = df.iloc[0]\n","    df = df[1:]\n","    df = df.loc[:, df.columns.notnull()]\n","    df = pd.DataFrame(df, columns=['SETTLEMENTDATE','INTERVENTION','REGIONID','RRP'])\n","    df['SETTLEMENTDATE']=pd.to_datetime(df['SETTLEMENTDATE'])\n","    df['DATE']= df['SETTLEMENTDATE'].dt.date\n","    df['RRP']=pd.to_numeric(df['RRP'])\n","    df['INTERVENTION'] = df['INTERVENTION'].astype(np.float64)\n","    df['PRIORITY'] =0\n","    df['PRIORITY'] = df['PRIORITY'].astype(np.int32)\n","    df['YEAR'] = df['SETTLEMENTDATE'].dt.year\n","    df['file'] = filename\n","    appended_data.append(df)\n","   except:\n","    pass\n"," appended_data = pd.concat(appended_data,ignore_index=True)\n"," return appended_data"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f56fcb2d-fcec-4f21-b76a-c455571e3d78"},{"cell_type":"code","source":["existing_files=spark.sql('select distinct file as file from price where PRIORITY = 0 and year= 2024').toPandas()['file'].tolist()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f96abc66-049b-4ad5-be9c-c6a5c57a08f2"},{"cell_type":"code","source":["###########################################\n","Path = \"/lakehouse/default/Files/1_Transform/0/Current/DispatchIS_Reports/\"\n","df = pd.read_parquet(Path+\"_log/log.parquet\")\n","df = df.replace(to_replace='None', value=np.nan).dropna()\n","list_files = df['file'].tolist()\n","filelist_csv = [w.replace('.zip', '.CSV') for w in list_files]\n","files_to_upload = list(set(filelist_csv) - set(existing_files))\n","files_to_upload = list(dict.fromkeys(files_to_upload))\n","print(len(files_to_upload))\n","########################### Write Data ##########################################################\n","if len(files_to_upload) >0 :\n"," df=spark.createDataFrame(extract_price(Path,files_to_upload))\n"," df.write.mode(\"append\").format(\"delta\").saveAsTable(\"price\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"17a6c9c1-e03c-41ec-8f57-266e31c4f30a"},{"cell_type":"markdown","source":["<mark>**Maintenance**</mark>"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c88a9b8c-af98-4ef5-8a03-ac820fde46f3"},{"cell_type":"code","source":["from pyspark.sql.functions import max\n","x = spark.sql('describe detail scada').select(max(\"numFiles\")).head()[0]\n","print(x)\n","if x > 50 :\n","    spark.sql('OPTIMIZE scada ' )\n","    spark.sql('OPTIMIZE price' )"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6427a8e6-b150-4810-8081-6fd703ef2c35"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"dependencies":{"lakehouse":{"default_lakehouse":"f8fd3d2f-5516-436f-9647-4d1d5879ea6f","default_lakehouse_name":"storage","default_lakehouse_workspace_id":"17a2fb56-2f22-4a10-b21b-d0b071d07a92"},"environment":{}}},"nbformat":4,"nbformat_minor":5}